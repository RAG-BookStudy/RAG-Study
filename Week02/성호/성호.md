# 성호 2주차 학습내용

## 토큰화 방식의 비교

토큰(token): 자연어 처리에서 텍스트를 처리하기 위해 나눈 작은 단위 <br>
토큰화(tokenize): 텍스트를 토큰으로 나누는 과정

## 기존 토큰화 방식 2가지

> - 문자 기반 토큰화: 텍스트를 개별 문자로 쪼개어 각 문를 하나의 토큰으로 취급하는 방식
>   - hello -> ['h', 'e', 'l', 'l', 'o']

> - 단어 기반 토큰화: 텍스트를 담위 단위로 사용
>   - Hello, World! -> ["Hello", ",", "world", "!"]

<br>

### LLM의 텍스트 생성 방식

먼저 나올 토큰을 생성 <br>
-> 그 다음 나올 토큰을 확률적으로 계산하여 생성하는 로직

### **각 토큰화 방식마다 너무 큰 단점들이 존재**

> - 문자기반 토큰화: 예측을 더 많이 해야함
>   - 비용상 비효율적 + 틀릴 확률 상승

> - 단어 기반 토큰화: 모든 단어를 저장한 '단어 사전'을 만들어야한다.
>   - 한 단어에서 나올 수 있는 모든 형태를 다 가지고 있어야하기에 비효율

<br>

## 서브워드 기반 토큰화

기존 토큰화 방식의 단점들을 보완하고자 문자와 단어의 중간점인 **"서브워드 기반 토큰화"** 가 등장

### **서브워드 기반 토큰화** <br>

: 두 가지의 토큰화 방식의 장단점을 보완한 방식으로 단어 전체를 토큰화 하는 대신 서브워드 활용

- **서브워드란?**<br>
  : 단어보다 작은단위 / 접두사, 접미사, 어근 등 해당

- **"바이트 페어 인코딩(BPE)"**<br>
  : 서브워드 방식 중 하나로 단어를 처음에 문자 단위로 분해 -> 자주 등장하는 문자 쌍을 결합해 더 큰 서브워드로 만드는 방식

※ 한글의 경우 토큰화로 나눠진 단위가 음절이 아닌 자음모음 수준에서 구분하므로 글자 수 기주으로 정확히 나누기 어려움 <br>
-> 실제로 내가 자소서를 1000자 이내로 적어달라고하면 정확하게 적어주지 못하는 경우가 대다수 <br> 개인 팁으로는 몇단락으로 적어줘. 라고 하면 된다.<br>

## 컨텍스트 윈도우(context window)

모델이 한 번에 처리하고 이해할 수 있는 전체 텍스트의 문맥 범위

- LLM이 입력과 출력을 합쳐 처리할 수 있는 **최대 토큰 수**
- 예: GPT-4 Turbo는 128K 토큰, Claude 3는 200K 토큰까지 지원
- 컨텍스트 윈도우가 클수록 더 긴 대화 기록이나 문서를 한 번에 참조 가능

### 컨텍스트 길이(context length)

컨텍스트 윈도우 내에서 **실제로 사용 중인 토큰의 수**

- 대화가 길어질수록 컨텍스트 길이가 증가
- 컨텍스트 길이가 윈도우를 초과하면 오래된 내용부터 잘려나감 (truncation)
- RAG에서는 검색된 문서가 컨텍스트 길이를 차지하므로 효율적인 청킹이 중요

### Context Window vs Context Length 차이점

| 구분     | Context Window                      | Context Length                |
| -------- | ----------------------------------- | ----------------------------- |
| **정의** | 모델이 처리할 수 있는 **최대 용량** | 현재 **실제로 사용 중인 양**  |
| **비유** | 물탱크의 **총 용량**                | 물탱크에 **채워진 물의 양**   |
| **특성** | 모델마다 고정된 값                  | 대화/입력에 따라 변동         |
| **예시** | Claude 3: 200K 토큰                 | 현재 대화에서 5K 토큰 사용 중 |

**쉬운 비유**

```
Context Window = 128K (고정)
┌────────────────────────────────────┐
│ ████████░░░░░░░░░░░░░░░░░░░░░░░░░░ │
│ ← Context Length: 30K (가변) →      │
└────────────────────────────────────┘
```

- **Window**: 한계선 (넘을 수 없음)
- **Length**: 현재 사용량 (계속 증가)

**핵심 차이**

- **Context Window**는 모델의 **스펙**(하드웨어적 한계)
- **Context Length**는 현재 대화의 **상태**(소프트웨어적 사용량)

대화가 길어져서 Context Length가 Context Window를 초과하면, 오래된 내용부터 잘려나가게 됨.

---

## LangChain 기초 (01-Basic)

### ChatOpenAI 모델

OpenAI의 채팅 전용 LLM 객체 생성 시 주요 옵션

> - `temperature`: 샘플링 온도 (0~2). 낮을수록 결정론적, 높을수록 창의적
> - `max_tokens`: 생성할 최대 토큰 수
> - `model_name`: 사용할 모델명 (gpt-4.1, gpt-4o 등)

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    temperature=0.1,  # 창의성 (0.0 ~ 2.0)
    model_name="gpt-4.1-nano",  # 모델명
)

# 질의
response = llm.invoke("대한민국의 수도는?")
print(response.content)
```

<br>

### LogProb (토큰 확률)

모델이 해당 토큰을 예측할 확률의 로그 값

```python
llm_with_logprob = ChatOpenAI(
    temperature=0.1,
    model_name="gpt-4.1-nano",
).bind(logprobs=True)

response = llm_with_logprob.invoke("질문")
print(response.response_metadata)  # logprobs 정보 포함
```

<br>

### 프롬프트 캐싱

반복되는 동일한 입력 토큰에 대한 비용 절감 기능

> - 1024 토큰 이상의 프롬프트에서 자동 활성화
> - 동일한 prefix 재사용 시 비용 50% 절감, 지연시간 80% 감소
> - 캐싱할 내용(시스템 프롬프트 등)을 앞에, 가변 내용을 뒤에 배치

<br>

---

## LCEL 인터페이스

LCEL(LangChain Expression Language): 체인을 쉽게 구성하기 위한 표현 언어

### Runnable 프로토콜의 주요 메서드

| 메서드 | 설명 |
| ------ | ---- |
| `stream` | 응답의 청크를 스트리밍 |
| `invoke` | 입력에 대해 체인 호출 |
| `batch` | 입력 목록에 대해 일괄 처리 |
| `astream` | 비동기 스트리밍 |
| `ainvoke` | 비동기 호출 |
| `abatch` | 비동기 배치 |

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI()
prompt = PromptTemplate.from_template("{topic} 에 대하여 설명해줘.")

# LCEL 파이프 연산자(|)로 체인 구성
chain = prompt | model | StrOutputParser()

# 호출
chain.invoke({"topic": "ChatGPT"})

# 스트리밍
for token in chain.stream({"topic": "멀티모달"}):
    print(token, end="", flush=True)

# 배치 처리
chain.batch([{"topic": "ChatGPT"}, {"topic": "Instagram"}])
```

<br>

---

## Runnable 컴포넌트

### RunnablePassthrough

입력을 변경하지 않거나 추가 키를 더하여 전달

```python
from langchain_core.runnables import RunnablePassthrough

# 단독 사용: 입력을 그대로 전달
RunnablePassthrough().invoke({"num": 10})  # {'num': 10}

# 체인에서 사용
runnable_chain = {"num": RunnablePassthrough()} | prompt | ChatOpenAI()
runnable_chain.invoke(10)
```

**RunnablePassthrough.assign()**: 입력값에 새로운 key/value 쌍 추가

```python
# 입력 키: num, 할당 키: new_num
(RunnablePassthrough.assign(new_num=lambda x: x["num"] * 3)).invoke({"num": 1})
# 결과: {'num': 1, 'new_num': 3}
```

<br>

### RunnableParallel

여러 Runnable을 병렬로 실행

```python
from langchain_core.runnables import RunnableParallel

runnable = RunnableParallel(
    passed=RunnablePassthrough(),  # 입력 그대로 전달
    extra=RunnablePassthrough.assign(mult=lambda x: x["num"] * 3),  # 추가 키
    modified=lambda x: x["num"] + 1,  # 람다 함수
)

runnable.invoke({"num": 1})
# 결과: {'passed': {'num': 1}, 'extra': {'num': 1, 'mult': 3}, 'modified': 2}
```

**체인에서의 병렬 처리 예시**

```python
chain1 = (
    {"country": RunnablePassthrough()}
    | PromptTemplate.from_template("{country} 의 수도는?")
    | ChatOpenAI()
)
chain2 = (
    {"country": RunnablePassthrough()}
    | PromptTemplate.from_template("{country} 의 면적은?")
    | ChatOpenAI()
)

combined_chain = RunnableParallel(capital=chain1, area=chain2)
combined_chain.invoke("대한민국")
# 수도와 면적을 동시에 질의
```

<br>

### RunnableLambda

사용자 정의 함수를 체인에 맵핑

```python
from langchain_core.runnables import RunnableLambda
from datetime import datetime

def get_today(a):
    return datetime.today().strftime("%b-%d")

prompt = PromptTemplate.from_template(
    "{today} 가 생일인 유명인 {n} 명을 나열하세요."
)

chain = (
    {"today": RunnableLambda(get_today), "n": RunnablePassthrough()}
    | prompt
    | ChatOpenAI()
    | StrOutputParser()
)

print(chain.invoke(3))
```

**itemgetter와 함께 사용**

```python
from operator import itemgetter

chain = (
    {
        "a": itemgetter("word1") | RunnableLambda(len),
        "b": itemgetter("word2") | RunnableLambda(len),
    }
    | prompt
    | model
)

chain.invoke({"word1": "hello", "word2": "world"})
```

<br>

※ LCEL의 핵심은 파이프(`|`) 연산자로 컴포넌트를 연결하여 체인을 구성하는 것 <br>
→ 가독성이 좋고, 병렬 처리 및 비동기 처리가 용이함

---

마사학 어플에 적용할 방법 생각:

- 카테고리 별로 벡터DB를 나누어서 생성한다.
  - 너무 세분화된 카테고리 X, 어느정도 범용성있는 카테고리
  -
- 이를 검색할때 해당 카테고리를 고른 후 검색을 한다.
