# 성호 1주차 학습내용

## 목차
- [Chapter 1. RAG 이해하기](#chapter-1-rag-이해하기)
  - [What is RAG?](#what-is-ragretrieval-augmented-generation)
  - [Why RAG?](#why-rag)
  - [LangChain을 이용한 RAG 시스템 구축](#langchain을-이용한-rag-시스템-구축)
- [궁금증 Q&A](#궁금증-qa)

---

## Chapter 1. RAG 이해하기

### What is RAG(Retrieval-Augmented Generation)?

> **RAG** = 검색(Retrieval) + 증강(Augmented) + 생성(Generation)

LLM 모델의 한계(특히 할루시네이션)를 보완하기 위한 기술이다.

**핵심 특징:**
- 문서로드, 검색, 답변 생성의 **투명한 과정**을 통해 할루시네이션 현상을 줄임
- 최신 정보와 신뢰할 수 있는 외부 데이터를 활용해 **응답 정확도 향상**
- DB와 모델을 커스터마이즈하여 **특정 도메인에 특화된 서비스**(like 챗봇) 구축 가능

---

### Why RAG?

RAG를 사용하는 4가지 이유:

| 이유 | 설명 |
|:---|:---|
| **최신 정보 반영** | 데이터베이스에 최신 정보를 넣으면 최신 정보가 반영됨 |
| **할루시네이션 방지** | 데이터베이스에 넣은 문서를 토대로 답변하기 때문에 환각 현상 방지 |
| **쉬운 구현** | 세부 모듈(임베딩, 텍스트 분할 등)을 플러그인처럼 교체하는 방식으로 구현 |
| **투명한 과정** | 전 과정을 모니터링하고 추적하여 각 단계를 상세히 분석하고 조정 가능 |

> 각 단계별로 어떤 데이터가 추출되었는지, 프롬프트로 인한 답변은 어떻게 나왔는지 모든 과정을 투명하게 볼 수 있어 이를 해석하고 분석하여 더 나은 답변이 나오도록 개선할 수 있다.

---

### LangChain을 이용한 RAG 시스템 구축

> **LangChain**: 대규모 언어 모델로 구동되는 애플리케이션을 개발하기 위한 프레임워크

**LangChain 비유:**
| 구성요소 | 역할 |
|:---|:---|
| **LLM (엔진)** | 엄청난 지능과 힘을 가졌지만, 엔진 혼자서는 굴러갈 수 없다 |
| **LangChain (차체, 바퀴, 핸들)** | 엔진의 힘을 바퀴로 전달하고(행동), 운전자가 원하는 방향으로 가게 해준다(조작) |

즉, LLM이라는 고성능 엔진을 가지고 실제로 굴러가는 **자동차(서비스)를 만들게 해주는 조립 키트**가 바로 랭체인이다.

**LangChain의 필요성:**
- ChatGPT 자체 RAG 시스템 → 세부 알고리즘 조정 **불가**
- LangChain을 통한 RAG → 모든 프로세스 하나하나 구현 및 **튜닝 가능**

---

#### RAG 시스템 전체 프로세스

```
질문 → RAG[문서 로드 → 텍스트 분할 → 임베딩 → 벡터스토어 → 리트리버] → Prompt → LLM → 답변
```

---

#### RAG 프로세스: 사전 단계 (1~4단계)

| 단계 | 이름 | 설명 |
|:---:|:---|:---|
| **1** | 문서로드 (Document Load) | 외부 데이터 소스에서 필요한 문서(PDF, 엑셀, 이미지, 논문 등)를 불러와서 초기 처리 |
| **2** | 텍스트 분할 (Text Split) | 로드된 문서를 처리 가능한 작은 단위인 **청크(Chunk)**로 분할 |
| **3** | 임베딩 (Embedding) | 분할된 청크를 **벡터 형태**로 변환하여 문서를 수치화 (컴퓨터가 이해할 수 있는 언어로 변경) |
| **4** | 벡터 스토어 저장 | 임베딩된 청크를 데이터베이스에 저장 (매번 임베딩 비용 방지) |

**텍스트 분할 상세:**
- 토큰 수 기준으로 청크 크기 지정 → 해당 분량의 청크로 분할
- **청크 오버랩(chunk overlap)**: 분할된 청크 끝부분에서 맥락이 이어질 수 있도록 일부 겹쳐서 분할
- 질문에 대해서 각각의 청크에 대해 유사도를 계산해 가장 관련성이 높은 청크 추출

---

#### RAG 프로세스: 실행 단계 (5~8단계)

| 단계 | 이름 | 설명 |
|:---:|:---|:---|
| **5** | 리트리버 (Retriever) | 질문을 벡터로 변환하고, 관련된 벡터를 벡터 DB에서 검색 |
| **6** | 프롬프트 (Prompt) | 검색된 문서(청크)를 바탕으로 LLM에게 전달할 명령 구성 |
| **7** | LLM (Large Language Model) | 프롬프트를 입력받아 실제 응답 생성 |
| **8** | 체인 생성 | LCEL 문법을 활용해 앞선 단계들을 하나로 묶어 RAG 파이프라인 구성 |

**리트리버 상세:**
- 코사인 유사도, MMR과 같은 알고리즘으로 질문과 가장 관련 깊은 단락 선별
- **k 값 설정**을 통해 선택할 청크의 수 조절 가능
- 리트리버 성능 = **전체 시스템의 응답 품질**

---

## 궁금증 Q&A

### Q1. RAG의 Retrieval은 인터넷 검색도 포함하는가?

| 구분 | 내용 |
|:---:|:---|
| **결론** | 리트리버에 어떤 데이터 소스를 연결하느냐에 따라 다르다 |
| **기본** | 내부 문서(벡터 스토어) 검색이 일반적 |
| **확장** | 웹 검색 API(Google, Bing 등) 연결 시 인터넷 검색도 가능 |

---

### Q2. RAG 사용 시 LLM은 기존 학습 지식을 제외하고 답변하는가?

| 구분 | 내용 |
|:---:|:---|
| **결론** | 아니다. 검색된 문서 + 기존 지식이 **함께 활용**된다 |
| **기본 동작** | RAG 컨텍스트와 LLM 사전 지식이 결합되어 답변 생성 |
| **제한 방법** | 프롬프트에 "제공된 문서만 기반으로 답변하라" 명시 가능 |
| **주의** | 프롬프트로 제한해도 LLM의 언어/추론 능력은 기존 학습에서 비롯됨 |

---
